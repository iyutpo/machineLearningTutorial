{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "np.random.seed(1)\n",
    "\n",
    "def zero_pad(X, pad):\n",
    "    \"\"\"\n",
    "    Pad with zeros all images of the dataset X. The padding is applied to the height and width of an image, \n",
    "    as illustrated in Figure 1.\n",
    "    \n",
    "    Argument:\n",
    "    X -- python numpy array of shape (m, n_H, n_W, n_C) representing a batch of m images\n",
    "    pad -- integer, amount of padding around each image on vertical and horizontal dimensions\n",
    "    \n",
    "    Returns:\n",
    "    X_pad -- padded image of shape (m, n_H + 2*pad, n_W + 2*pad, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line)\n",
    "    X_pad = np.pad(X, ((0, 0), (pad, pad), (pad, pad), (0, 0)), 'constant', constant_values=(0, 0))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return X_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_single_step(a_slice_prev, W, b):\n",
    "    \"\"\"\n",
    "    Apply one filter defined by parameters W on a single slice (a_slice_prev) of the output activation \n",
    "    of the previous layer.\n",
    "    \n",
    "    Arguments:\n",
    "    a_slice_prev -- slice of input data of shape (f, f, n_C_prev)\n",
    "    W -- Weight parameters contained in a window - matrix of shape (f, f, n_C_prev)\n",
    "    b -- Bias parameters contained in a window - matrix of shape (1, 1, 1)\n",
    "    \n",
    "    Returns:\n",
    "    Z -- a scalar value, result of convolving the sliding window (W, b) on a slice x of the input data\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    # Element-wise product between a_slice and W. Do not add the bias yet.\n",
    "    s = a_slice_prev * W\n",
    "    # Sum over all entries of the volume s.\n",
    "    Z = np.sum(s)\n",
    "    # Add bias b to Z. Cast b to a float() so that Z results in a scalar value.\n",
    "    Z = float(Z + b)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return Z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_forward(A_prev, W, b, hparameters):\n",
    "    \"\"\"\n",
    "    Implements the forward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- output activations of the previous layer, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    W -- Weights, numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    b -- Biases, numpy array of shape (1, 1, 1, n_C)\n",
    "    hparameters -- python dictionary containing \"stride\" and \"pad\"\n",
    "        \n",
    "    Returns:\n",
    "    Z -- conv output, numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward() function\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from A_prev's shape (≈1 line)  \n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = np.shape(A_prev)\n",
    "    \n",
    "    # Retrieve dimensions from W's shape (≈1 line)\n",
    "    (f, f, n_C_prev, n_C) = np.shape(W)\n",
    "    \n",
    "    # Retrieve information from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters['stride']\n",
    "    pad = hparameters['pad']\n",
    "    \n",
    "    # Compute the dimensions of the CONV output volume using the formula given above. Hint: use int() to floor. (≈2 lines)\n",
    "    n_H = int((n_H_prev - f + 2 * pad) / stride) + 1\n",
    "    n_W = int((n_W_prev - f + 2 * pad) / stride) + 1\n",
    "    \n",
    "    # Initialize the output volume Z with zeros. (≈1 line)\n",
    "\n",
    "    Z = np.zeros((m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Create A_prev_pad by padding A_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "\n",
    "    for i in range(m):                                  # loop over the batch of training examples\n",
    "        a_prev_pad = A_prev_pad[i,:,:,:]                # Select ith training example's padded activation\n",
    "        for h in range(n_H):                           # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):                       # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):                   # loop over channels (= #filters) of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h * stride\n",
    "                    vert_end = h * stride+ f\n",
    "                    horiz_start = w * stride\n",
    "                    horiz_end = w * stride + f\n",
    "                    \n",
    "                    # Use the corners to define the (3D) slice of a_prev_pad (See Hint above the cell). (≈1 line)\n",
    "                    a_slice_prev = a_prev_pad[vert_start:vert_end,horiz_start:horiz_end,:]\n",
    "                    \n",
    "                    # Convolve the (3D) slice with the correct filter W and bias b, to get back one output neuron. (≈1 line)\n",
    "                    Z[i, h, w, c] = conv_single_step(a_slice_prev,W[:,:,:,c], b[:,:,:,c])\n",
    "             \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(Z.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    # Save information in \"cache\" for the backprop\n",
    "    cache = (A_prev, W, b, hparameters)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the forward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    A_prev -- Input data, numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    hparameters -- python dictionary containing \"f\" and \"stride\"\n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of the pool layer, a numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache used in the backward pass of the pooling layer, contains the input and hparameters \n",
    "    \"\"\"\n",
    "    \n",
    "    # Retrieve dimensions from the input shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\"\n",
    "    f = hparameters[\"f\"]\n",
    "    stride = hparameters[\"stride\"]\n",
    "    \n",
    "    # Define the dimensions of the output\n",
    "    n_H = int(1 + (n_H_prev - f) / stride)\n",
    "    n_W = int(1 + (n_W_prev - f) / stride)\n",
    "    n_C = n_C_prev\n",
    "    \n",
    "    # Initialize output matrix A\n",
    "    A = np.zeros((m, n_H, n_W, n_C))              \n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    for i in range(m):                         # loop over the training examples\n",
    "        for h in range(n_H):                     # loop on the vertical axis of the output volume\n",
    "            # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "            vert_start = h*stride\n",
    "            vert_end = vert_start + f\n",
    "            for w in range(n_W):                 # loop on the horizontal axis of the output volume\n",
    "                # Find the vertical start and end of the current \"slice\" (≈2 lines)\n",
    "                horiz_start = w*stride\n",
    "                horiz_end = horiz_start + f\n",
    "                \n",
    "                for c in range (n_C):            # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Use the corners to define the current slice on the ith training example of A_prev, channel c. (≈1 line)\n",
    "                    a_prev_slice = A_prev[i, vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                    \n",
    "                    # Compute the pooling operation on the slice. \n",
    "                    # Use an if statement to differentiate the modes. \n",
    "                    # Use np.max and np.mean.\n",
    "                    if mode == \"max\":\n",
    "                        A[i, h, w, c] = np.max(a_prev_slice)\n",
    "                    elif mode == \"average\":\n",
    "                        A[i, h, w, c] = np.mean(a_prev_slice)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Store the input and hparameters in \"cache\" for pool_backward()\n",
    "    cache = (A_prev, hparameters)\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(A.shape == (m, n_H, n_W, n_C))\n",
    "    \n",
    "    return A, cache\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def conv_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a convolution function\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of the cost with respect to the output of the conv layer (Z), numpy array of shape (m, n_H, n_W, n_C)\n",
    "    cache -- cache of values needed for the conv_backward(), output of conv_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of the cost with respect to the input of the conv layer (A_prev),\n",
    "               numpy array of shape (m, n_H_prev, n_W_prev, n_C_prev)\n",
    "    dW -- gradient of the cost with respect to the weights of the conv layer (W)\n",
    "          numpy array of shape (f, f, n_C_prev, n_C)\n",
    "    db -- gradient of the cost with respect to the biases of the conv layer (b)\n",
    "          numpy array of shape (1, 1, 1, n_C)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve information from \"cache\"\n",
    "    (A_prev, W, b, hparameters) = cache\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape\n",
    "    (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
    "    \n",
    "    # Retrieve dimensions from W's shape\n",
    "    (f, f, n_C_prev, n_C) = W.shape\n",
    "    \n",
    "    # Retrieve information from \"hparameters\"\n",
    "    stride = hparameters[\"stride\"]\n",
    "    pad = hparameters[\"pad\"]\n",
    "    \n",
    "    # Retrieve dimensions from dZ's shape\n",
    "    (m, n_H, n_W, n_C) = dZ.shape\n",
    "    \n",
    "    # Initialize dA_prev, dW, db with the correct shapes\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))                           \n",
    "    dW = np.zeros((f, f, n_C_prev, n_C))\n",
    "    db = np.zeros((1, 1, 1, n_C))\n",
    "\n",
    "    # Pad A_prev and dA_prev\n",
    "    A_prev_pad = zero_pad(A_prev, pad)\n",
    "    dA_prev_pad = zero_pad(dA_prev, pad)\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select ith training example from A_prev_pad and dA_prev_pad\n",
    "        a_prev_pad = A_prev_pad[i]\n",
    "        da_prev_pad = dA_prev_pad[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop over vertical axis of the output volume\n",
    "            for w in range(n_W):               # loop over horizontal axis of the output volume\n",
    "                for c in range(n_C):           # loop over the channels of the output volume\n",
    "                    \n",
    "                    # Find the corners of the current \"slice\"\n",
    "                    vert_start = h*stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w*stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, h, w, c]\n",
    "                    dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
    "                    db[:,:,:,c] += dZ[i, h, w, c]\n",
    "                    \n",
    "        # Set the ith training example's dA_prev to the unpadded da_prev_pad (Hint: use X[pad:-pad, pad:-pad, :])\n",
    "        dA_prev[i, :, :, :] = da_prev_pad[pad:-pad, pad:-pad, :]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask_from_window(x):\n",
    "    \"\"\"\n",
    "    Creates a mask from an input matrix x, to identify the max entry of x.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- Array of shape (f, f)\n",
    "    \n",
    "    Returns:\n",
    "    mask -- Array of the same shape as window, contains a True at the position corresponding to the max entry of x.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈1 line)\n",
    "    mask = (x==np.max(x))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return mask\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distribute_value(dz, shape):\n",
    "    \"\"\"\n",
    "    Distributes the input value in the matrix of dimension shape\n",
    "    \n",
    "    Arguments:\n",
    "    dz -- input scalar\n",
    "    shape -- the shape (n_H, n_W) of the output matrix for which we want to distribute the value of dz\n",
    "    \n",
    "    Returns:\n",
    "    a -- Array of size (n_H, n_W) for which we distributed the value of dz\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    # Retrieve dimensions from shape (≈1 line)\n",
    "    (n_H, n_W) = shape\n",
    "    \n",
    "    # Compute the value to distribute on the matrix (≈1 line)\n",
    "    average = dz / (n_H*n_W)\n",
    "    \n",
    "    # Create a matrix where every entry is the \"average\" value (≈1 line)\n",
    "    a = np.ones(shape) * average\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    return a\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pool_backward(dA, cache, mode = \"max\"):\n",
    "    \"\"\"\n",
    "    Implements the backward pass of the pooling layer\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- gradient of cost with respect to the output of the pooling layer, same shape as A\n",
    "    cache -- cache output from the forward pass of the pooling layer, contains the layer's input and hparameters \n",
    "    mode -- the pooling mode you would like to use, defined as a string (\"max\" or \"average\")\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- gradient of cost with respect to the input of the pooling layer, same shape as A_prev\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    \n",
    "    # Retrieve information from cache (≈1 line)\n",
    "    (A_prev, hparameters) = cache\n",
    "    \n",
    "    # Retrieve hyperparameters from \"hparameters\" (≈2 lines)\n",
    "    stride = hparameters[\"stride\"]\n",
    "    f = hparameters[\"f\"]\n",
    "    \n",
    "    # Retrieve dimensions from A_prev's shape and dA's shape (≈2 lines)\n",
    "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
    "    m, n_H, n_W, n_C = dA.shape\n",
    "    \n",
    "    # Initialize dA_prev with zeros (≈1 line)\n",
    "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
    "    \n",
    "    for i in range(m):                       # loop over the training examples\n",
    "        \n",
    "        # select training example from A_prev (≈1 line)\n",
    "        a_prev = A_prev[i]\n",
    "        \n",
    "        for h in range(n_H):                   # loop on the vertical axis\n",
    "            for w in range(n_W):               # loop on the horizontal axis\n",
    "                for c in range(n_C):           # loop over the channels (depth)\n",
    "                    # Find the corners of the current \"slice\" (≈4 lines)\n",
    "                    vert_start = h*stride\n",
    "                    vert_end = vert_start + f\n",
    "                    horiz_start = w*stride\n",
    "                    horiz_end = horiz_start + f\n",
    "                    \n",
    "                    # Compute the backward propagation in both modes.\n",
    "                    if mode == \"max\":\n",
    "                        \n",
    "                        # Use the corners and \"c\" to define the current slice from a_prev (≈1 line)\n",
    "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
    "                        # Create the mask from a_prev_slice (≈1 line)\n",
    "                        mask = create_mask_from_window(a_prev_slice)\n",
    "                        # Set dA_prev to be dA_prev + (the mask multiplied by the correct entry of dA) (≈1 line)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += np.multiply(mask, dA[i, h, w, c])\n",
    "                        \n",
    "                    elif mode == \"average\":\n",
    "                        \n",
    "                        # Get the value a from dA (≈1 line)\n",
    "                        da = dA[i, h, w, c]\n",
    "                        # Define the shape of the filter as fxf (≈1 line)\n",
    "                        shape = (f, f)\n",
    "                        # Distribute it to get the correct slice of dA_prev. i.e. Add the distributed value of da. (≈1 line)\n",
    "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += distribute_value(da, shape)\n",
    "                        \n",
    "    ### END CODE ###\n",
    "    \n",
    "    # Making sure your output shape is correct\n",
    "    assert(dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "base_dir = './tmp/cats_and_dogs_filtered'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "\n",
    "train_cat_fnames = os.listdir(train_cats_dir)\n",
    "train_dog_fnames = os.listdir(train_dogs_dir)\n",
    "train_dog_fnames.sort()\n",
    "\n",
    "\n",
    "\n",
    "image = np.array(Image.open(os.path.join(train_cats_dir, train_cat_fnames[1]) ))\n",
    "imagetest = np.zeros((1024, 1024, 3))\n",
    "imagetest[:image.shape[0], :image.shape[1], :] = image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 1024, 1024, 3)\n"
     ]
    }
   ],
   "source": [
    "def padding(size=20):\n",
    "    '''\n",
    "    size --> default 20\n",
    "    \n",
    "    images --> 4 dimensional np array\n",
    "    '''\n",
    "    images = np.zeros((size, 1024, 1024, 3))\n",
    "    for i in range(size//2):\n",
    "        cat = np.array(Image.open(os.path.join(train_cats_dir, train_cat_fnames[i])))\n",
    "        dog = np.array(Image.open(os.path.join(train_dogs_dir, train_dog_fnames[i])))\n",
    "        images[i, :cat.shape[0], :cat.shape[1], :] = cat\n",
    "        images[i+size//2, :dog.shape[0], :dog.shape[1], :] = dog\n",
    "    return images\n",
    "\n",
    "print(padding().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape = (4, 3, 3, 2)\n",
      "x_pad.shape = (40, 1024, 1024, 3)\n",
      "x[1,1] = [[ 0.90085595 -0.68372786]\n",
      " [-0.12289023 -0.93576943]\n",
      " [-0.26788808  0.53035547]]\n",
      "x_pad[1,1] = [[40. 45. 41.]\n",
      " [40. 45. 41.]\n",
      " [40. 44. 43.]\n",
      " ...\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1af3b89f788>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU4AAACuCAYAAABZYORfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAlAklEQVR4nO2deZBc13Wfv9PrdPfsC7YhIIAWIIpkKJtCKClWaLrE2BSjCvWHlKIdm7RNFcsuyUviVERZZTtlF0XGFS+UZcmlSLaoULbCaLGgRLJEK1psl2ESXEQSAiEsHAxADgbAYLZG790nf7x3H980ZoDp6cZ0D/p8VVP9+r37+t43aPzm3HvPIqqKYRiGsXoi7R6AYRjGRsOE0zAMo0FMOA3DMBrEhNMwDKNBTDgNwzAaxITTMAyjQUw4DcNoCBH5BRH5h3aPo52YcBqGYTSICadhGEaDmHB2ECLyIyJyXkRu9t9vE5FzInJbe0dmdBJr+Z6IyHdE5CEReVJE5kXkKyIyHLr+v0XktH/teyJyQ+jaiIjsE5EFEXkS+JEr+HgbAhPODkJVjwEfBD4nImngL4HPqOp32jowo6No4ntyD/BLwDagAnw0dO3rwG5gE/AM8LnQtT8DCsBW//5fav4pNjZiseqdh4jsA3YBCvxLVS22eUhGB9LI90REvgPsV9UH/PfXA88BKVWt1rUdBGaBQSCLJ5r/QlVf8q9/BLhVVd/e2ifaOJjF2Zn8D+BG4E9NNI1L0Oj35GTo+AQQB0ZFJCoiD4vIMRFZACb8NqPAGBBb5t6uxoSzwxCRXuBPgE8D/zW8DmUYjjV+T7aHjncAZeAc8LPAXcDtwACw03UDnMWb1tff29WYcHYejwBPq+r7gP8L/Hmbx2N0Jmv5nvyciFzvr4v+HvAFf5reBxSBGSANfMTd4F//Ep44p/0p/r2tfZSNhwlnByEidwF3AL/sn/pPwM0i8h/aNyqj02jie/I/gc8Ap4Ee4Nf885/Fm36/AvwA2F933weAXv++z+BtRnU1tjlkGF2Avzn0mKp+qt1juRowi9MwDKNBmhJOERkWkSdE5Ij/OrRCuwkReUFEnhORA830aRgrISJ3iMhhETkqIg+0ezzrjYhkV/j51+0e29VGU1N1EfkD4LyqPux/UYdU9YPLtJsA9qrquTV3ZhiXQESiwA+BfwOcAp4CfkZVf9DWgRlXJc1O1e8CHvWPHwXe3eTnGcZauQU4qqrHVbUEfB7v+2kYLadZ4dysqlMA/uumFdop8E0ReVpE7m+yT8NYjnGWOmmf8s8ZRsuJXa6BiPwdsGWZSx9uoJ8fV9VXRWQT8ISIvKSq31uhv/uB+wEymcyb9+zZ00A37eHZZ59t9xBWzete97p2D+GyzMzMsLi4KA3etlz7i9ahwt+vVCr15muvvRYRQUSIRCKuDSISHNdTKpVQVaLRKAC1Wo1isUihUCAWi1Gr1RARBgcHgzaNsJblMxFZ9r7686q6ob6vV5Bzqjq21psvK5yqevtK10RkWkS2quqUiGwFzqzwGa/6r2dE5Mt406plhVNVPwl8EuDmm2/W7373u5d/ijbT39/f7iGsmt/+7d9u9xAuy+///u+v5bZTLI1uuQZ4tb5R+Pt1/fXX6+OPP04qlSKZTJJMJonFYsTj8UBwnIg6AU0mkywuLpLL5SgWi4gIuVyO5557jhMnTrBjxw52797N9PQ04+Pj3HTTTcuKrz+WJa+uz1qtFvTtiEQi1Gq1iz7D3eOuh8ccFsxarRb8ZDKZhn6xVylNhY02O1Xfx2tRBPcCX6lvICIZEelzx8BPAS822a9h1PMUsFtEdolIArgb7/u5IiISWITFYpFcLseFCxfIZrOUSiVqtRrVajVoC5612dvbSyaToVQqkc1muXDhAuPj42zbto1z587R39/PW97yFrZv304+n0dVL7IGw+/DwuzaunGVSiUmJycD0XTnw/eEiUajSyxoJ5b1fRrNcVmL8zI8DDwuIvcBk8B7wcsPCHxKVe8ENgNf9v+RY8BfqerfNtmvYSxBVSsi8gHgG0AU+AtVPbjKewG4cOEC6XQ6mG5HIhFEhEqlElih0WiUcrlMb28vAwMDHD58GPBmHT/6oz/K6OjoEtE6d+4cO3bsWNLP5cbi2lUqFXp6elBVCoUCmUzmIpENLyksZ6HWW81rWTowLqYp4VTVGeAdy5x/FbjTPz4OvKmZfgxjNajq14CvNXhPIC6xWIxCoUAkEiEajQbi6Y7DU+BKpcLw8DCvf/3rOXXqFPF4nEQiARAIZ6VSIRaL8eKLL7Jnz54lSwArERa/SqXC4uIilUqFhYUFent7KZfLS9ZiwRNJ95m1Wu0iq9T9AQhb0EZzNGtxGsaGx60rFotFkskkqkq1Wl0iSCJCIpFAVYnFXvtvMzw8TDQa5dSpU+TzeSqVCpFIhJ6eHgYHB+nr62N4eJhcLsfAwMAlx+AE0dHT08PZs2eJRCLBuMJrryutezrqp/LhDTCjOUw4ja5FVSmVSgBLBMldcxanO+/EtFqtBtPeSCTC2NhYYK2+8sorRKPRYG1zbGwMVSWdTgfT/rCA1e94u/VNEWF+fp4f/vCHpFIp4vE4AwMDF40xLI7OKg5fqxdKm6q3BhNOo2txlmWtViOXywXi6NYww6LkroE3NXaC6l57enro7+9HRDh37hwzMzOICKOjo/T09ASC66zVcrm8rIg58atUKlQqFTZv3szg4CCbNm2iUCgEywFOcMPTcLek4MYb/rywoBrNY8JpdDVOWPr7+4lGo8Tj8WBd0113ouest/A03gmXW7/csmULQ0NDDA4OcvToUUZGRti6dSvgWXu5XI5UKrVkug+vWbzxeJxKpUK5XOaFF16gXC6TSCSoVqv09vYGwlgqlUgkEkvWMeun+/VuTpdbXzVWjwmn0dU4UXTT4zBh/0jXDggswrDrjzuuVCokEgnGx8fp7++nWCxSLpeJx+MAxOPxi6bYrq/wT7VaJZvN0tvbSyQSCUQyGo1SrVaDNU9373J+myaUVw4TTqOruZSP40rrnE68nFDWT90dfX199Pf3B22dlVjfNyyN8BERTp8+vWRDyolpuI37jPAmVvhc2EoGLmpnrB3bYjO6lnrHdCeU9W3gtcgb1yYsYmFRDX9usejVT3NWq7M6VZVyubxEtOun1eVymenpaWZmZigUClQqlUuOp3689Z+33LMZa8csTqOrqRea+mN4zYIDgs0kJ0guRDNsgcZisWB67e51QheLxZibm6O3tzewXN3GTjQaZXZ2lj/+4z9mfHycTZs2MTIyEqy3hn1Ow5tCsLxPZzQaXSLK4SgiozlMOI2uZrm1RmdRhjeJ3DX3Wm+tus2eSCQSbOjU+1mG28RisaCvqakpfvCDHxCLxYhGo7z97W9nYmKCWCwWJBRxO+phoQwLqRPzcLRQ+PpyYzbWjgmn0bXUhyCGLcnw1Da8A+7Eyf2cOnWKTCbD6OgoqVQq8OHMZrPBhtHJkyfJZDLs3LmTWq0WrHu6/srlMsViMRA1FytfLBYZGhoKdvud9Vq/fhmOFnIsZ0m7e43mMeE0upqwxVm/sw0sm+jDtXXWXTabJZvNUqvVGBkZYXBwkEQiwYkTJ7hw4QKJRIK5ubklyTvcrnyxWKRWqwVCmclkqNVq9PX1BdamC9904uli6cOhoE7kwxtZ9ZjF2TpMOI2uJuyU7sQwnDjDEZ72ht+PjY1x+PBhstksIyMjnD59mvPnz1MoFJicnCSXy1EoFLjtttsol8tks1mSySRzc3PMzc1RKpWIx+Ns3749EOhsNsvo6Cjz8/NBdFO1Wg3WUet3+sNjDe/su5345ZYbjOZoid0ulymSJR4f9a8/LyI3t6Jfw7hSLLdT7XBrn07EKpUKIyMjJBIJSqVSkJijt7eXTZs2MTw8TKVSIZ/PUyqVWFhYIJVKAQS5PUulEoODgwDcdNNNbN++nW3btpHP54P8mW4nP7yMsJyVHN4oClujRuto2uIUr0jWnxEqkiUi+3Rpkax3Arv9n7cAn/BfDaOtLCeK9bj1TOeEHr6nWq1y7bXXkkwmqVQqnD59mvn5ed70pjdRKpWCHxervnnzZhYXF0kmk2QyGRYWFohGowwMDLC4uMi2bdsC8U2lUvT09HDy5Mlgp95Znc6/04057KBfnwwEWLKBZDRPK6bqQZEsABFxRbLCwnkX8Fn1/qX3i8ig+JnjW9C/YTRFvfO4c+MJ46a7boocDsk8efIkk5OTnDt3jnQ6jaoyMTHB2bNnufXWWxkYGGBmZgbwnOKr1Sqzs7PE43EGBwe5cOEC4G1CLS4ukslkSKVSQYq7wcHBYLru1jXr1zTdc9RP4d1Y3XqsTdVbQyuEc7kiWfXW5EqFtEw4jbZTnwDYsVyCjHrHcueveeONN5LL5Ugmk0FM+jXXXMOZM2eYm5tjfn6egwcPUqvVKBQKTE1NsXv3bs6cORNs/CSTSWZnZ4MQzYWFBSqVSmCxlkqlwPKMxWIXrc2Gxxx2Swpnj7fsSK2hFWucqymStapCWuAV0xKRAyJy4Nw5K8Pejfip174tIodE5KCI/DqAiAyLyBMicsR/HXL3iMiH/DX0wyLy06vtq35q7gSnnuWmuPF4nGg0ytjYGNVqleHhYfr6+qjVamzZsoW5uTleeukljh8/zrPPPsvAwABPPvkkU1NTnDhxglwux9zcHPl8nnQ6HYikqtLf388111wTiGM485ETyuWsSOcFUCqVgrR37ndq1mbraIXFuZoiWasqpAUXF2trwfiMjclvquoz4tWrelpEngB+AfiWqj7sb0I+AHxQRK7HqzF0A7AN+DsR2aOql013HhaT5XJbhq208I42vObInkwm2bx5MzMzM5w+fTrYPCqXy5TLZQ4ePMh1111HoVCgWCxy8ODBoN/R0dFAAHt7ewGCDEkDAwMMDQ1RrVbJ5XJkMpnA/9P1717duFyNoqeeeop0Os3IyAipVIrNmzeTSqUumUzZWD2tEM6gSBbwCt4X+Gfr2uwDPuCvf74FmLf1TWMl/KiaZwBUdVFEDuEt7dwF3OY3exT4DvBB//znVbUIvCwiR/HW3v/pcn0tl4bN7V47X8t8Ph+sHYbXESuVSuAmtLi4CMDs7CxHjhzh9ttvp1wuMzY2xq5du5iamqJQKNDb28v4+DhvfOMbg/XLXC4XhGpWKhWy2Sx9fX1MT09TLBZJJBKBNRqOOKr34XSuVYODg2QyGQ4dOkQ+n6dQKARrpC7m3WiOpoVTVyiSJSK/7F//c7w6MHcCR4Ec8IvN9mt0ByKyE/gx4J+Bze4PrnolqTf5zcaB/aHb3Br6ZXFi+corr3DgwAEWFxeX1A4K76K7qa9L1uGszdnZWXbv3s2OHTtYWFhg586d/OM//iOZTIZcLkc0GuW6665j06ZNpFIpMpkM4+PjzM7OMjk5yfz8fJAE2Yn0sWPHiEajDA4OMjc3x8TEBD09PWzbto1t27aRSqWoVCqkUqkl65muZPHY2BinTp0K4tqj0SjZbJb5+fnm/kEMoEUO8LpMkSxfMN2xAu9vRV9G9yAivcAXgd9Q1YVLuNI0tIYO3A+wZcuW4Hy5XL6ojk94Y6g+1NJlQ8rn8wwNDbFr1y4OHTrEgQMHmJ6e5s1vfjPbt28nk8lQLpfp6+sjnU6TSCQol8vMz89z9OhRZmZmmJ+fJ5fLMTk5CcA111zD9u3bufHGGwE4ffo0zz//PEeOHOGll14KxHf37t2MjIwwNjZGX18fvb29ZLNZVDU4XywWg8TILnTTaB6LHDI6EhGJ44nm51T1S/7paefGJiJbgTP++TWtob/xjW/U8LQ3FotRLBaXuB6Fxdodx+Nxent7ed/73hck5jh48CDHjx8H4NZbbyWVSjE7OxtMzYeHh8nn88zMzJDL5Th27BiTk5NMTk4SjUZ59dVXGR4e5iMf+QgDAwNLpt8nTpxgeno68Nd0uT0PHz4crMGG/Tv37NnD+Pg4mzdvZnZ2llwuF1idzsneaA4TTqPj8KfGnwYOqeofhS7tA+4FHvZfvxI6/1ci8kd4m0O7gSdX21c4rVu5XF4x1juc5f2+++6jr6+PcrkMwAsvvMDp06fZtGkT5XKZXbt2kclkgvf5fJ7Tp09z5MgRLly4QDab5dSpU7z88suUy2Xm5ub4xje+ERRzc2N56KGHOHv2LENDngNBOJzShVQ6XEjnV7/6VU6ePMnZs2cDR/t4PB5U5DSax4TT6Dj8BMA/D7wgIs/5p38LTzAfF5H7gEngvQD+mvrjeEEXFeD9q9lR9+8NxM9N1d2mUNhn050X8eqv79+/P1h/fP7553nqqac4f/48fX19fOADH+Anf/InA/E9e/Ysjz32GE8//TQLCwtBko6FhYWgVtHY2Bh/+Id/yI4dOxgbG6NWq5FOp0mlUgwPDwebUm4sQOAQH97wcYKaTqcZHR0ln88zNzdHoVDgyJEjQXJlozlMOI2Oo6enB1VdaUHzHcudVNUHgQcb7ctZl06QyuXyRWnbwu/dbvr+/fuDGPJcLsfY2Bi5XA5VZXR0dEnykAMHDrB//35yuVzQZyQSCdyPwrWLnNBFIhGy2eySMEsn5OFkyY5wSGU0GqWnpydY00wmk5TLZXK5HPl8vtFfkbEMJpxGVxN2IE8mk0tcfMLHzrJzYuXCIavVarAu6jIaPfTQQ+zZs4drr72WV199lcnJyUCQRYRkMkksFiOXywUx7M7B3bkmuUTIboxOKN1SgXOUD5cLnpqa4vDhw5RKJXp6egJr1tU66u/vJ51Ot+cXfZVhwml0LeGsQs7VyFmU4aS/9UXVwnk43e66s/IKhQLRaJTp6WkWFxeJRCLMzs4Cr0UaORFMJpMXCaSrReTSzYlfelhVmZyc5Nvf/nYg5PF4nIGBAeLxOOVymVgsRjqdJp1OB6I/PDzMyMgIO3bsYGhoiMOHD/PlL395/X/ZVxkmnEZX4yzLsI9moVCgWq0Ga59uTTIskmFhrVQqVCqVINzR+VK6ImuFQmFJ4uFw7XY3RXcCGRZrtzwwMTHB8ePHyWazFAoFAFKpFKVSiampKdLpNJFIJChH7J6jVCpx5swZjh8/zvPPP8/8/LxZnC3ChNPoetxU2O2s14c0up3r+jIbzufTRQ+58he1Wi1YSwxPq527UNj9KdxPOBWcqlcls1AocOrUKaanp4nH4yQSCYrFYlCaI5FIkMvl6O3tZW5ujlgsRn9/P/39/UHG+FQqRblcJplMsrCwsG6/16sZE06jqwnXGYpEIqTTac6fP08sFgs2ZeA1H85w5UgnqC6U0YVqugxIbn3RWanOCb2npyewWMM75eGyw27dc3Fxkfn5eS5cuBCU03Ax7+HpeaFQIJlMAgRLDc66rHfcN5rHhNPoWsLp2BxuwyecxzJcrC3sN+lwWeBdAg43dXdTezctj8fjQaw5vJbmLSyU8FrEUrVa5ezZs0xNTZFMJsnn84HV6iKASqVS4ODukooUCoUlYw5HQlmxttZgwml0NWFLzGU7ctNaJ6AuHDO8M+7au2l6uVwOrMtwWeGwSIb9QsPuReFXt2suIiwsLFAoFAKLMpFI0NPTE+zMA0FJjsXFRfL5fODDmc/n6enpIZvNBrk7rcpl6zDhNLoaZ0GGLUtnNQJLpupuOu0Ezr13lp4Tz/Dnug0gWBr1494vJ57ufDiZiPMRdZtIbhzVapVMJrNESJ3Qu6xIbs01mUzaVL1FtEQ4ReQO4BG87EifUtWH667fhhce97J/6kuq+nut6Nsw1oqbqrs1SDc9d0XXnEA567C+AqazSJ3VGU4sXJ+uzmWK7+npCazGcP/AEid7N70eHx+nUCgwNzcXiHM6nV5SvbJcLgfrseE1WCegLlrIRLN1rFexNoC/V9V3NdufYbQKJ3rOdzORSAR+li7Kpz7JR1gQXRhmeHPIWa5hH9HwNNlN+Vf6PAgy4AdWr9vsccK9uLgYOM47XJtweKjzCXVWr3tOo3nWq1ibYXQk4WigcD0fl3HdWaHLWZ3hRMZOgN1uPCzd5HHnc7lc4K8ZLvzmPteto7pKl07Ak8kk/f39FAqFoJxw/YZS+I9AoVBYMtUPC7nRPOtVrA3gbSLyfbx0X/9ZVQ+2oG/DWDNOCMPi6JITh62z8M53WHychRnOrB7OrBR+deV8wwIXtkLrP7darQZuR5VKhWKxiKvB5QTTlSR2Yu9cnsJ1h8KO9a5vo3laIZyrSSL7DPA6Vc2KyJ3A3+Cl/rr4w0KJZnfs2EFfX18Lhnhluffee9s9hFVz++23t3sIl+WRRx5Zl37CFqQTMucX6dYIncN5faYkYMkGTTQaDdqFLUgXNul8OMObP8tNnV1RNTedD7s2JRIJdu3axdatWzl//jwnTpwIKmS6sEwnkC5aqd4/1CzO1rAuxdpUdSF0/DUR+biIjKrqRWUsw4lm9+7da//KxhXFbao4wXHx5G5TpT7/pbMiw5tA9W3CO+Ruk8cJXHja7yzW5eoeubBJ59rk1k7f8IY38PrXv558Ps/evXtZWFjgqaeeYm5ujkQiETxPeNffiXO9z6qxdtalWJuIbAGmVVVF5Ba8ssQzLejbMNaMs+xisdiStUggsCCdn6drDwTWYNh6DIdVhi29SqUS7KI70Q1Pl8Mp65zAuam/E0uX6ejChQt89atfZXh4mJ/4iZ/gpptuore3l507dzIzM8ORI0c4fPgws7Ozwf2uT4saai3rVaztPcCviEgFyAN3q80ZjDbjhDPspO7WOV0MeDiSyImjI5wD093nYtOd6EWjUQqFAn19fYFghi1Ul1vTvQ+7J7kdfrd84AR8dnaWffv2ceDAAYaGhoLSGefPnw82l9znuXGaaLaW9SrW9jHgY63oy+gefFe3A8ArqvouERkG/hewE5gA/r2qzvptPwTcB1SBX1PVb1zu8ycmJrjnnnuu0OivPMeOHWv3ELoWi78yOplfBw6F3j8AfEtVdwPf8t8jItfjLRHdANwBfNwXXcO4IphwGh2JiFwD/FvgU6HTdwGP+sePAu8Onf+8qhZV9WXgKJ5/sWFcEUw4jU7lT4D/AoS3gjer6hSA/7rJP7+cL/H4OozR6FJMOI2Ow0+2e0ZVn17lLavxJfYaitwvIgdE5MBax2cYlh3J6Dj8MMN/5wdL9AD9IvIYMC0iW1V1SkS2Amf8Wy7rS+wI+wmLiHl2GGvCLE6j49iyZQuqeo2q7sTb9Pl/qvpzwD7AhWndi5dxC//83SKS9P2JdwNPrvOwjS7CLE5jI/Ew8LiI3AdMAu8F8P2GH8dLLFMB3q+qFpRtXDFMOI2ORlW/A3zHP54B3rFCuweBB9dtYEZXY1N1wzCMBjHhNAzDaBATTsMwjAYx4TQMw2gQE07DMIwGaYlwishfiMgZEXlxhesiIh8VkaMi8ryI3NyKfg3DMNpBqyzOz+BlpVmJd+I5Je/GK4vxiRb1axiGse60RDhV9XvA+Us0uQv4rHrsBwb9kDnDMIwNx3qtca46e004CcPZs2fXZXCGYRiNsF7CuersNar6SVXdq6p7x8bGrvCwDMMwGme9hHPV2WsMwzA6nfUSzn3APf7u+luBeZeQ1jAMY6PRkiQfIvLXwG3AqIicAn4XiENQtO1rwJ14JQ1ywC+2ol/DMIx20Koqlz9zmesKvL8VfRmGYbQbixwyDMNoEBNOwzCMBjHhNDoSERkUkS+IyEsickhE3iYiwyLyhIgc8V+HQu0/5If0HhaRn27n2I2rHxNOo1N5BPhbVb0OeBNwCHgA+Jaq7ga+5b9HRK7Hq010A17o78dFJNqWURtdgQmn0XFUq1WAW4FPA6hqSVXn8EJ3H/WbPQq82z++C/i8qhZV9WU8741b1nHIRpdhwml0HKVSCeAs8Jci8qyIfEpEMsBm5//rv27yb1lTSO8VewDjqseE0+g4PO81bgY+oao/BlzAn5avwJpCepseqNG1mHAaHUc8Hgc4par/7J/6Ap6QTrusWv7rGf+6hfQa64oJp9Fx+MJ5UkTe4J96B17N9H3Avf65e4Gv+Mf7gLtFJCkiu/Dyvj65fiM2ug2rq250Kr8KfE5EEsBxvDDdCPC4iNwHTALvBVDVgyLyOJ64VoD3q2q1PcM2ugETTqMjUdXngOXWId+xQvsHgQev5JgMw2FTdcMwjAZZr2Jtt4nIvIg85//8Tiv6NQzDaAetmqp/BvgY8NlLtPl7VX1Xi/ozDMNoG+tVrM0wDOOqYT3XON8mIt8Xka+LyA3r2K9hGEZLWa9d9WeA16lqVkTuBP4Gz9fuIkTkfrza60QiEbZs2bJOQ1w7jz32WLuHsGruuOOOdg/hskxMTLR7CIZxSdbF4lTVBVXN+sdfA+IiMrpC2yAkLhKxTX/DMDqPdVEmEdkiIuIf3+L3O7MefRuGYbSa9SrW9h7gV0SkAuSBu/06RIZhGBuO9SrW9jE8dyXDMIwNjy0iGoZhNIgJp2EYRoOYcBqGYTSICadhGEaDmHAahmE0iAmn0ZGIyH8UkYMi8qKI/LWI9FhddaNTMOE0Og7fxffXgL2qeiMQxaubbnXVjY7AhNPoVGJASkRiQBqv+JrVVTc6AhNOo+Pwo3P/O15doSlgXlW/SQvqqhtGKzDhNDoOf6p+F7AL2AZkROTnLnHLquuqi8j9InJARA40PVCjazHhNDqOWq0G8LKqnlXVMvAl4F/Rgrrq4exbV2r8xtWPCafRcfhT9beKSNrPqvUO4BBWV93oEJpO8iEi2/FqDW0BasAnVfWRujYCPALcCeSAX1DVZ5rt27g68fOwfgEvAXYFeBb4JNCL1VU3OoBWZEeqAL+pqs+ISB/wtIg8oao/CLV5J54VsBt4C/AJ/9UwlkVVfxcvPWGYIlZX3egAmp6qq+qUsx5VdRFvSlW/o3kX8Fn12A8MurUqwzCMjUZL1zhFZCfwY8A/110ydxHDMK4aWlasTUR6gS8Cv6GqC/WXl7llRXcRQsXaDMMwOo2WKJOIxPFE83Oq+qVlmqzJXcSE0zCMTqRpZfJ3zD8NHFLVP1qh2T7gHvF4K14kyFSzfRuGYbSDVkzVfxz4eeAFEXnOP/dbwA4IirV9Dc8V6SieO9IvtqBfwzCMttC0cKrqP7D8Gma4jQLvb7YvwzCMTsAWEQ3DMBrEhNMwDKNBTDgNwzAaxITTMAyjQUw4DcMwGsSE0zAMo0FMOA3DMBrEhNMwDKNBTDgNwzAaxITTaBtTU1McOXKE48ePB+eq1SqlUgkROSIiT4jIkLsmIh8SkaMiclhEfjp0/s0i8oJ/7aN+/gTDuGKYcBptY2BggO3bty85NzMzQyQSQVV3A98CHgAQkeuBu4EbgDuAj4tI1L/tE3ipCF2VgTvW5wmMbsWE02gb6XT6opyr2WyWaNTpIY8C7/aP7wI+r6pFVX0ZL2HMLX4lgX5V/Sc/J8JnQ/cYxhWhFWnltovIt0XkkIgcFJFfX6bNbSIyLyLP+T+/02y/xtVJpVJxVS7xUw9u8i+tVEVg3D+uP28YV4z1KtYG8Peq+q4W9Gd0JytVEVh1dQFYWmHAMNbKehVrM4xVEYvF8Gbc4E/Dz/iXVqoicMo/rj+/LOEKA60ct9FdrFexNoC3icj3ReTrInJDK/s1rh56e3upVoOS6PcCX/GP9wF3i0hSRHbhbQI96U/nF0Xkrf5u+j2hewzjiiDur3vTH+QVa/su8GB93SER6QdqqpoVkTuBR/xd0+U+JzyVegNwuCUDfI1R4FyLP/NK0A3j3AX04S0ZVfAsxVm8nfMTwCTwXlU9DyAiHwZ+yW/7G6r6df/8XuAzQAr4OvCruoovtogs0vrv10Zgo3y3Wk34uV+nqmNr/aCWCKdfrO3/AN+4RN2hcPsJYK+qrvs/nogc2AjTNBvnlWcjj70Z7LmbZ12KtYnIFueULCK3+P3ONNu3YRhGO1ivYm3vAX5FRCpAHrh7NVMpwzCMTmS9irV9DPhYs321iE+2ewCrxMZ55dnIY28Ge+4madnmkGEYRrdgIZeGYRgN0jXCKSJ3+Fl1jorIA+0ez0qIyF+IyBkRebHdY7kUqwm17VQ2yndhrYjIhJ8t6jkROeCfG/azTa0661Sns9z/lbU855qya6nqVf8DRIFjwLVAAvg+cH27x7XCWG8FbgZebPdYLjPOrcDN/nEf8MNO/Z1u1O9CE884AYzWnfsD4AH/+AHgv/nH1/u/gySeX+0xINruZ1jlc170f2Utzwk8CbwNb6/m68A7L9d3t1ictwBHVfW4qpaAz+Nl2+k4VPV7wPl2j+Ny6MYNtd0w34UWcxdetilYRdap9R9e46zwf6Wh51xrdq1uEc6VMusYLeAyobadRjd8FxT4pog87UfiAWxWLzwVXV3WqY1Ko8+5puxarfDj3Ag0lEHHWD1+qO0X8UIgF9o9nlXQDd+FH1fVV0VkE/CEiLx0ibbd8PuAFmXXcnSLxblSZh2jCfxQ2y8Cn9O6/AQdzFX/XVDVV/3XM8CX8abe0/60dLVZpzYqjT5nQ9m1HN0inE8Bu0Vkl4gk8Eow7GvzmDY0qwm17VCu6u+CiGT8vLiISAb4KeBFvGe812922axT6zvqltLQc+pas2u1e2dsHXfg7sTb+T0GfLjd47nEOP8amALKeH8N72v3mFYY59vxpjTPA8/5P3e2e1xX03dhjc92Ld7u8feBg+75gBG8Gk5H/Nfh0D0f9n8Xh1nFjnKn/Cz3f2UtzwnsxfvjcgwvwlEu17dFDhmGYTRIt0zVDcMwWoYJp2EYRoOYcBqGYTSICadhGEaDmHAahmE0iAmnYRhGg5hwGoZhNIgJp2EYRoP8f9oAlW5TCuojAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "# x = np.random.randn(4, 3, 3, 2)\n",
    "# x_pad = zero_pad(x, 2)\n",
    "x_pad = padding(40)\n",
    "print (\"x.shape =\", x.shape)\n",
    "print (\"x_pad.shape =\", x_pad.shape)\n",
    "print (\"x[1,1] =\", x[1,1])\n",
    "print (\"x_pad[1,1] =\", x_pad[1,1])\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = -6.999089450680221\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "a_slice_prev = np.random.randn(4, 4, 3)\n",
    "W = np.random.randn(4, 4, 3)\n",
    "b = np.random.randn(1, 1, 1)\n",
    "\n",
    "Z = conv_single_step(a_slice_prev, W, b)\n",
    "print(\"Z =\", Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean = 0.048995203528855794\n",
      "Z[3,2,1] = [-0.61490741 -6.7439236  -2.55153897  1.75698377  3.56208902  0.53036437\n",
      "  5.18531798  8.75898442]\n",
      "cache_conv[0][1][2][3] = [-0.20075807  0.18656139  0.41005165]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "print(\"Z's mean =\", np.mean(Z))\n",
    "print(\"Z[3,2,1] =\", Z[3,2,1])\n",
    "print(\"cache_conv[0][1][2][3] =\", cache_conv[0][1][2][3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "A = [[[[1.74481176 0.86540763 1.13376944]]]\n",
      "\n",
      "\n",
      " [[[1.13162939 1.51981682 2.18557541]]]]\n",
      "\n",
      "mode = average\n",
      "A = [[[[ 0.02105773 -0.20328806 -0.40389855]]]\n",
      "\n",
      "\n",
      " [[[-0.22154621  0.51716526  0.48155844]]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 4, 4, 3)\n",
    "hparameters = {\"stride\" : 2, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A =\", A)\n",
    "print()\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A =\", A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_mean = 1.4524377775388075\n",
      "dW_mean = 1.7269914583139097\n",
      "db_mean = 7.839232564616838\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "dA, dW, db = conv_backward(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =  [[ 1.62434536 -0.61175641 -0.52817175]\n",
      " [-1.07296862  0.86540763 -2.3015387 ]]\n",
      "mask =  [[ True False False]\n",
      " [False False False]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "x = np.random.randn(2,3)\n",
    "mask = create_mask_from_window(x)\n",
    "print('x = ', x)\n",
    "print(\"mask = \", mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distributed value = [[0.5 0.5]\n",
      " [0.5 0.5]]\n"
     ]
    }
   ],
   "source": [
    "a = distribute_value(2, (2,2))\n",
    "print('distributed value =', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mode = max\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.          0.        ]\n",
      " [ 5.05844394 -1.68282702]\n",
      " [ 0.          0.        ]]\n",
      "\n",
      "mode = average\n",
      "mean of dA =  0.14571390272918056\n",
      "dA_prev[1,1] =  [[ 0.08485462  0.2787552 ]\n",
      " [ 1.26461098 -0.25749373]\n",
      " [ 1.17975636 -0.53624893]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "hparameters = {\"stride\" : 1, \"f\": 2}\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "\n",
    "dA_prev = pool_backward(dA, cache, mode = \"max\")\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])  \n",
    "print()\n",
    "dA_prev = pool_backward(dA, cache, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', dA_prev[1,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
